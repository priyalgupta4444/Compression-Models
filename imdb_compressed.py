# -*- coding: utf-8 -*-
"""IMDB_compressed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vua4Jz6yjJk5blHdsnxqs6Ml6AYSoB8F

#Preprocessing
"""

# âœ… Install Dependencies and fix fsspec conflict
!pip install -q transformers datasets==2.19.1 scikit-learn fsspec==2023.12.2

import os
os.kill(os.getpid(), 9)  # ðŸ” Restart runtime immediately after install

import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    DistilBertTokenizer, DistilBertForSequenceClassification,
)
from datasets import load_dataset
from sklearn.metrics import accuracy_score
import random
from torch.optim import AdamW # Import AdamW from torch.optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from datasets import load_dataset, concatenate_datasets, Dataset
from sklearn.model_selection import train_test_split
import pandas as pd

# Load IMDb
dataset = load_dataset("imdb")

# Select 500 examples of each class
negatives = dataset["train"].filter(lambda x: x["label"] == 0).shuffle(seed=42).select(range(500))
positives = dataset["train"].filter(lambda x: x["label"] == 1).shuffle(seed=42).select(range(500))
balanced_data = concatenate_datasets([negatives, positives]).shuffle(seed=42)

# Convert to pandas for stratified splitting
df = pd.DataFrame(balanced_data)

# Stratified split
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

# Convert back to Hugging Face Datasets
train_data = Dataset.from_pandas(train_df.reset_index(drop=True))
val_data = Dataset.from_pandas(val_df.reset_index(drop=True))

# Test set (optional)
test_data = dataset["test"].shuffle(seed=42).select(range(200))

# prompt: for the above generated validation and train set check the number of rows for each class

print("Train set class distribution:")
print(train_data['label'].count(0))
print(train_data['label'].count(1))

print("\nValidation set class distribution:")
print(val_data['label'].count(0))
print(val_data['label'].count(1))

# prompt: can you show the total number of rows in test train and validation set

print(f"\nTotal number of rows in train set: {len(train_data)}")
print(f"Total number of rows in validation set: {len(val_data)}")
print(f"Total number of rows in test set: {len(test_data)}")

dataset

# âœ… Tokenization Function
def tokenize_data(tokenizer, data):
    return data.map(lambda x: tokenizer(x['text'], padding="longest", truncation=True), batched=True)

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self.input_ids = data["input_ids"]
        self.attention_mask = data["attention_mask"]
        self.labels = data["label"]

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx]),
            "attention_mask": torch.tensor(self.attention_mask[idx]),
        }, torch.tensor(self.labels[idx])

    def __len__(self):
        return len(self.labels)

# âœ… Evaluation Function
def evaluate(model, dataloader):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            inputs, targets = batch
            inputs = {k: v.to(device) for k, v in inputs.items()}
            outputs = model(**inputs).logits
            preds.extend(outputs.argmax(dim=1).cpu().numpy())
            labels.extend(targets.numpy())
    return accuracy_score(labels, preds)

def train(model, train_loader, val_loader, test_loader, epochs=5, patience=3):
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    best_val_accuracy = -1
    epochs_no_improve = 0

    for epoch in range(epochs):
        model.train()
        for batch in train_loader:
            inputs, targets = batch
            inputs = {k: v.to(device) for k, v in inputs.items()}
            targets = targets.to(device)

            outputs = model(**inputs, labels=targets)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        # Evaluate on validation set
        val_acc = evaluate(model, val_loader)
        print(f"Epoch {epoch+1} Validation Accuracy: {val_acc:.4f}")

        # âœ… Save model if validation improves
        if val_acc > best_val_accuracy:
            best_val_accuracy = val_acc
            epochs_no_improve = 0
            torch.save(model.state_dict(), 'best_model.pth')  # âœ… save best model
        else:
            epochs_no_improve += 1
            if epochs_no_improve == patience:
                print(f"Early stopping after {epoch+1} epochs.")
                break

    # âœ… Load the best model before testing
    model.load_state_dict(torch.load('best_model.pth'))

    # Evaluate on test set
    test_acc = evaluate(model, test_loader)
    print(f"Final Test Accuracy (Best Model): {test_acc:.4f}")

"""#Bert Base"""

# âœ… Baseline 1: BERT-base with Validation and Early Stopping
print("ðŸ”¹ Baseline 1: BERT-base with Validation and Early Stopping")
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
train_tok = tokenize_data(bert_tokenizer, train_data)
val_tok = tokenize_data(bert_tokenizer, val_data)
test_tok = tokenize_data(bert_tokenizer, test_data)

train_ds = IMDbDataset(train_tok)
val_ds = IMDbDataset(val_tok)
test_ds = IMDbDataset(test_tok)

train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=16)
test_dl = DataLoader(test_ds, batch_size=16)


bert_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
train(bert_model, train_dl, val_dl, test_dl)

# Check the number of layers in the BERT model
print(f"Number of BERT layers: {bert_model.bert.config.num_hidden_layers}")

# prompt: check te weights and biasses of each layer i the given bert model

# Function to print weights and biases for each layer
def print_layer_weights_biases(model):
    print("\nChecking weights and biases for each layer:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Layer: {name}, Shape: {param.shape}")
            # Optionally print a small part of the data
            # print(f"  Weights/Biases sample: {param.data.cpu().numpy().flatten()[:5]}...")


print_layer_weights_biases(bert_model)

"""# compression:"""

from transformers import BertTokenizer, BertForSequenceClassification
from torch.nn.functional import cosine_similarity
import torch
import pickle
from torch.utils.data import DataLoader
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… Load trained model (classifier)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", output_hidden_states=True)
model.load_state_dict(torch.load("best_model.pth"))
model.to(device)
model.eval()

# âœ… Use tokenizer and dataloader (already prepared)
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

similarity_matrices = []

with torch.no_grad():
    for batch in test_dl:
        inputs, _ = batch  # ignore labels
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Forward pass with hidden states
        outputs = model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states  # Tuple of (13, batch, seq_len, hidden)

        # Average over batch (optional if batch_size=1)
        hidden = torch.stack(hidden_states)  # shape: (13, B, L, H)
        hidden = hidden.squeeze(1) if hidden.shape[1] == 1 else hidden.mean(dim=1)  # shape: (13, L, H)

        seq_len = hidden.shape[1]
        sim_mat = torch.zeros((13, 13)).to(device)

        for i in range(13):
            for j in range(i, 13):
                sim_sum = 0.0
                for token_idx in range(seq_len):
                    sim = cosine_similarity(hidden[i, token_idx], hidden[j, token_idx], dim=-1)
                    sim_sum += sim.item()
                avg_sim = sim_sum / seq_len
                sim_mat[i, j] = sim_mat[j, i] = round(avg_sim, 2)

        similarity_matrices.append(sim_mat.cpu().numpy())

# âœ… Save similarity matrices
with open('imdb_sim_matrices.pkl', 'wb') as f:
    pickle.dump(similarity_matrices, f)

print(f"Saved similarity matrices for {len(similarity_matrices)} sentences.")

import pickle
import seaborn as sns
import matplotlib.pyplot as plt

# Load similarity matrix (assume average over all sentences)
with open('imdb_sim_matrices.pkl', 'rb') as f:
    sim_matrices = pickle.load(f)

# Average similarity matrix across all samples
avg_sim_matrix = sum(sim_matrices) / len(sim_matrices)

x_axis_labels = y_axis_labels = ['embed'] + [f'enc{i}' for i in range(1,13)]

sns.heatmap(avg_sim_matrix, annot=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cmap='coolwarm')
plt.title("Average Layer Similarity Heatmap")
plt.savefig("imdb_similarity_heatmap.png", dpi=300, bbox_inches='tight')
plt.show()
plt.close()

thres = 0.9  # similarity threshold to prune redundant layers

ind_layer_to_del = set()

for i in range(len(avg_sim_matrix)):
    if i in ind_layer_to_del:
        continue
    for k in range(i + 1, len(avg_sim_matrix)):
        if k in ind_layer_to_del:
            continue
        if avg_sim_matrix[i][k] >= thres:
            ind_layer_to_del.add(k)
            print(f"Pruning layer {k} due to high similarity with layer {i} (sim={avg_sim_matrix[i][k]:.2f})")

print("Layers to delete:", sorted(ind_layer_to_del))

from transformers import BertForSequenceClassification
import torch.nn as nn

# Load your trained model again
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
model.load_state_dict(torch.load("best_model.pth"))
model.to(device)
model.eval()

# Convert encoder layers to a list
enc_list = list(model.bert.encoder.layer)

# Sort indices descending to delete without index shift issues
for idx in sorted(ind_layer_to_del, reverse=True):
    del enc_list[idx]

# Replace model's encoder layers with the pruned list
model.bert.encoder.layer = nn.ModuleList(enc_list)

# Adjust config for new number of layers
model.bert.config.num_hidden_layers = len(enc_list)

print(f"New number of encoder layers: {model.bert.config.num_hidden_layers}")

model.save_pretrained('./compressed_bert_imdb')
print("Compressed model saved to ./compressed_bert_imdb")

"""# After compression fine tuning"""

# prompt: on the above compressed model, train it again (fine tuning)

# ðŸ”¹ Fine-tuning the Compressed Model
print("\nðŸ”¹ Fine-tuning the Compressed Model:")

# Load the compressed model
compressed_model = BertForSequenceClassification.from_pretrained('./compressed_bert_imdb')
compressed_model.to(device)

# Set up training hyperparameters for fine-tuning
# Use fewer epochs and potentially a smaller learning rate for fine-tuning
finetune_epochs = 3
finetune_lr = 1e-5
finetune_patience = 2 # Less patience for quicker convergence or early stopping

# Re-initialize AdamW optimizer for the compressed model
optimizer = AdamW(compressed_model.parameters(), lr=finetune_lr)

# Start fine-tuning
# Re-using the train function with potentially different hyperparameters
# The train function already handles loading the best model based on validation accuracy
train(compressed_model, train_dl, val_dl, test_dl, epochs=finetune_epochs, patience=finetune_patience)

# Evaluate the fine-tuned compressed model on validation and test sets
print("ðŸ”¹ Evaluating Fine-tuned Compressed Model:")

# The 'train' function already loads the best fine-tuned model and prints the final test accuracy.
# We can also explicitly evaluate the model loaded by the train function if needed.
# Assuming the 'train' function saves the best fine-tuned model to 'best_model.pth' (or a new name for finetuning).
# If you want to save the fine-tuned model to a different name, modify the train function or save it here.
# For simplicity, we assume the train function saved the best finetuned model to 'best_model.pth' and the last printed test accuracy is for this model.

# If you want to explicitly evaluate the best finetuned model after the train function finishes:
# Load the best finetuned model state dict
# compressed_model.load_state_dict(torch.load('best_model.pth')) # Already done inside train function at the end
# compressed_model.to(device)

# val_acc_finetuned = evaluate(compressed_model, val_dl)
# print(f"Validation Accuracy (Fine-tuned Compressed Model): {val_acc_finetuned:.4f}")

# test_acc_finetuned = evaluate(compressed_model, test_dl) # Already printed by the train function
# print(f"Test Accuracy (Fine-tuned Compressed Model): {test_acc_finetuned:.4f}")

# Optionally, save the fine-tuned compressed model
# compressed_model.save_pretrained('./finetuned_compressed_bert_imdb')
# print("Fine-tuned compressed model saved to ./finetuned_compressed_bert_imdb")

"""# fine tuning using freeze and unfreeze //wrong method

"""

# prompt: fine tune baseline 1; bert base model

# âœ… Fine-tuning: Unfreeze the last 4 layers of BERT and add a lower learning rate
print("\n\nðŸ”¹ Fine-tuning: Unfreeze last 6 layers, lower LR")

# Re-initialize the model to ensure a clean state for fine-tuning
fine_tune_bert_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Freeze all layers first
for param in fine_tune_bert_model.bert.parameters():
    param.requires_grad = False

# Unfreeze the last 4 encoder layers
num_encoder_layers = fine_tune_bert_model.bert.config.num_hidden_layers
unfreeze_layers = 6 # Number of layers to unfreeze from the end
for i in range(num_encoder_layers - unfreeze_layers, num_encoder_layers):
    for param in fine_tune_bert_model.bert.encoder.layer[i].parameters():
        param.requires_grad = True

# Ensure the classifier layer is trainable
for param in fine_tune_bert_model.classifier.parameters():
     param.requires_grad = True

# Verify which layers are trainable
print("\nTrainable parameters after freezing/unfreezing:")
for name, param in fine_tune_bert_model.named_parameters():
    if param.requires_grad:
        print(f"  {name} (Trainable)")

# Fine-tuning training with a lower learning rate
print("\nStarting fine-tuning training...")
train(fine_tune_bert_model, train_dl, val_dl, test_dl, epochs=5)

print_layer_weights_biases(fine_tune_bert_model)

import argparse
import numpy as np
import torch
from sklearn.model_selection import train_test_split
import gc
import numpy as np
import torch.optim as optim
import torch.nn as nn
import string
from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig, BertTokenizer, BertModel, AutoTokenizer, AutoModelForTokenClassification
import pandas as pd
from torch.utils.data import TensorDataset, DataLoader, Dataset
import random
import os
import time
from torch import optim
from safetensors.torch import load_file,save_file
from torch.nn.functional import cosine_similarity
import pickle



"""# Distil Bert"""

print("\nðŸ”¹ Baseline 2: DistilBERT with Validation and Early Stopping")
distil_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
train_tok = tokenize_data(distil_tokenizer, train_data)
val_tok = tokenize_data(distil_tokenizer, val_data)
test_tok = tokenize_data(distil_tokenizer, test_data)

train_ds = IMDbDataset(train_tok)
val_ds = IMDbDataset(val_tok)
test_ds = IMDbDataset(test_tok)

train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=16)
test_dl = DataLoader(test_ds, batch_size=16)

distil_model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
train(distil_model, train_dl, val_dl, test_dl)

# prompt: for the above implemented distil bert model, check the number of layers and the weight and bias of each and every layer

# Check the number of layers in the DistilBERT model
print(f"\nNumber of DistilBERT layers: {distil_model.distilbert.config.n_layers}")

# Function to print weights and biases for each layer
def print_layer_weights_biases(model):
    print("\nChecking weights and biases for each layer:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Layer: {name}, Shape: {param.shape}")
            # Optionally print a small part of the data
            # print(f"  Weights/Biases sample: {param.data.cpu().numpy().flatten()[:5]}...")

print_layer_weights_biases(distil_model)

"""# Random Layer deletion"""

print("\nðŸ”¹ Baseline 3: BERT with Random Layer Deletion with Validation and Early Stopping")
compressed_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Remove random encoder layers
num_layers_to_remove = 6
all_layers = list(range(12))
layers_to_remove = sorted(random.sample(all_layers, num_layers_to_remove), reverse=True)
for i in layers_to_remove:
    del compressed_model.bert.encoder.layer[i]

train(compressed_model, train_dl, val_dl, test_dl)

print("âœ… All baselines completed.")

# prompt: For the above random layer deletion bert model, check the number of layers and the weigths and biases of each layer

# Check the number of layers in the compressed BERT model
print(f"\nNumber of Compressed BERT layers: {len(compressed_model.bert.encoder.layer)}")

# Print weights and biases for the compressed BERT model
print_layer_weights_biases(compressed_model)

"""# No finetuning model"""

from transformers import AutoTokenizer, TFAutoModel
import tensorflow as tf
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Load tokenizer and frozen BERT base
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
bert = TFAutoModel.from_pretrained("bert-base-uncased", trainable=False)

# Helper function to get [CLS] embeddings
def get_cls_embeddings(dataset, batch_size=32):
    cls_embeddings = []
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        inputs = tokenizer(batch["text"], padding=True, truncation=True, return_tensors="tf")
        outputs = bert(**inputs)
        cls_batch = outputs.last_hidden_state[:, 0, :]  # CLS token
        cls_embeddings.append(cls_batch.numpy())
    return np.vstack(cls_embeddings)

# Extract embeddings and labels
X_train = get_cls_embeddings(train_data)
y_train = np.array(train_data["label"])

X_val = get_cls_embeddings(val_data)
y_val = np.array(val_data["label"])

X_test = get_cls_embeddings(test_data)
y_test = np.array(test_data["label"])

# Train classifier (no BERT training)
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

# Evaluate
val_preds = clf.predict(X_val)
val_acc = accuracy_score(y_val, val_preds)
print("Validation Accuracy (Frozen BERT):", val_acc)

test_preds = clf.predict(X_test)
test_acc = accuracy_score(y_test, test_preds)
print("Test Accuracy (Frozen BERT):", test_acc)
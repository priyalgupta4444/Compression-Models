# -*- coding: utf-8 -*-
"""IMDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vua4Jz6yjJk5blHdsnxqs6Ml6AYSoB8F

#Preprocessing
"""

# ✅ Fix fsspec conflict & keep everything stable
!pip install -q datasets==2.18.0 fsspec==2023.12.2 --no-deps
import os
os.kill(os.getpid(), 9)  # 🔁 Restart runtime immediately after install

# ✅ Install Dependencies
!pip install transformers datasets scikit-learn -q

import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    DistilBertTokenizer, DistilBertForSequenceClassification,
    # Removed AdamW from transformers import
)
from datasets import load_dataset
from sklearn.metrics import accuracy_score
import random

# Import AdamW from torch.optim
from torch.optim import AdamW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from datasets import load_dataset

# ✅ Load and reduce IMDB dataset
dataset = load_dataset("imdb")

# Use 800 train, 200 validation, and 200 test samples
train_val_data = dataset["train"].shuffle(seed=42).select(range(1000))
test_data = dataset["test"].shuffle(seed=42).select(range(200))

# Split the training data into train and validation
train_val_dataset_dict = train_val_data.train_test_split(test_size=0.2, seed=42)
train_data = train_val_dataset_dict['train']
val_data = train_val_dataset_dict['test']

dataset

# ✅ Tokenization Function
def tokenize_data(tokenizer, data):
    return data.map(lambda x: tokenizer(x['text'], padding="longest", truncation=True), batched=True)

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self.input_ids = data["input_ids"]
        self.attention_mask = data["attention_mask"]
        self.labels = data["label"]

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx]),
            "attention_mask": torch.tensor(self.attention_mask[idx]),
        }, torch.tensor(self.labels[idx])

    def __len__(self):
        return len(self.labels)

# ✅ Evaluation Function
def evaluate(model, dataloader):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            inputs, targets = batch
            inputs = {k: v.to(device) for k, v in inputs.items()}
            outputs = model(**inputs).logits
            preds.extend(outputs.argmax(dim=1).cpu().numpy())
            labels.extend(targets.numpy())
    return accuracy_score(labels, preds)

def train(model, train_loader, val_loader, test_loader, epochs=5, patience=3):
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    best_val_accuracy = -1
    epochs_no_improve = 0

    for epoch in range(epochs):
        model.train()
        for batch in train_loader:
            inputs, targets = batch
            inputs = {k: v.to(device) for k, v in inputs.items()}
            targets = targets.to(device)

            outputs = model(**inputs, labels=targets)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        # Evaluate on validation set
        val_acc = evaluate(model, val_loader)
        print(f"Epoch {epoch+1} Validation Accuracy: {val_acc:.4f}")

        # ✅ Save model if validation improves
        if val_acc > best_val_accuracy:
            best_val_accuracy = val_acc
            epochs_no_improve = 0
            torch.save(model.state_dict(), 'best_model.pth')  # ✅ save best model
        else:
            epochs_no_improve += 1
            if epochs_no_improve == patience:
                print(f"Early stopping after {epoch+1} epochs.")
                break

    # ✅ Load the best model before testing
    model.load_state_dict(torch.load('best_model.pth'))

    # Evaluate on test set
    test_acc = evaluate(model, test_loader)
    print(f"Final Test Accuracy (Best Model): {test_acc:.4f}")

"""#Bert Base"""

# ✅ Baseline 1: BERT-base with Validation and Early Stopping
print("🔹 Baseline 1: BERT-base with Validation and Early Stopping")
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
train_tok = tokenize_data(bert_tokenizer, train_data)
val_tok = tokenize_data(bert_tokenizer, val_data)
test_tok = tokenize_data(bert_tokenizer, test_data)

train_ds = IMDbDataset(train_tok)
val_ds = IMDbDataset(val_tok)
test_ds = IMDbDataset(test_tok)

train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=16)
test_dl = DataLoader(test_ds, batch_size=16)


bert_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
train(bert_model, train_dl, val_dl, test_dl)

# Check the number of layers in the BERT model
print(f"Number of BERT layers: {bert_model.bert.config.num_hidden_layers}")

# prompt: check te weights and biasses of each layer i the given bert model

# Function to print weights and biases for each layer
def print_layer_weights_biases(model):
    print("\nChecking weights and biases for each layer:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Layer: {name}, Shape: {param.shape}")
            # Optionally print a small part of the data
            # print(f"  Weights/Biases sample: {param.data.cpu().numpy().flatten()[:5]}...")


print_layer_weights_biases(bert_model)

"""# compression:"""

from transformers import BertTokenizer, BertForSequenceClassification
from torch.nn.functional import cosine_similarity
import torch
import pickle
from torch.utils.data import DataLoader
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ✅ Load trained model (classifier)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", output_hidden_states=True)
model.load_state_dict(torch.load("best_model.pth"))
model.to(device)
model.eval()

# ✅ Use tokenizer and dataloader (already prepared)
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

similarity_matrices = []

with torch.no_grad():
    for batch in test_dl:
        inputs, _ = batch  # ignore labels
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Forward pass with hidden states
        outputs = model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states  # Tuple of (13, batch, seq_len, hidden)

        # Average over batch (optional if batch_size=1)
        hidden = torch.stack(hidden_states)  # shape: (13, B, L, H)
        hidden = hidden.squeeze(1) if hidden.shape[1] == 1 else hidden.mean(dim=1)  # shape: (13, L, H)

        seq_len = hidden.shape[1]
        sim_mat = torch.zeros((13, 13)).to(device)

        for i in range(13):
            for j in range(i, 13):
                sim_sum = 0.0
                for token_idx in range(seq_len):
                    sim = cosine_similarity(hidden[i, token_idx], hidden[j, token_idx], dim=-1)
                    sim_sum += sim.item()
                avg_sim = sim_sum / seq_len
                sim_mat[i, j] = sim_mat[j, i] = round(avg_sim, 2)

        similarity_matrices.append(sim_mat.cpu().numpy())

# ✅ Save similarity matrices
with open('imdb_sim_matrices.pkl', 'wb') as f:
    pickle.dump(similarity_matrices, f)

print(f"Saved similarity matrices for {len(similarity_matrices)} sentences.")

import pickle
import seaborn as sns
import matplotlib.pyplot as plt

# Load similarity matrix (assume average over all sentences)
with open('imdb_sim_matrices.pkl', 'rb') as f:
    sim_matrices = pickle.load(f)

# Average similarity matrix across all samples
avg_sim_matrix = sum(sim_matrices) / len(sim_matrices)

x_axis_labels = y_axis_labels = ['embed'] + [f'enc{i}' for i in range(1,13)]

sns.heatmap(avg_sim_matrix, annot=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cmap='coolwarm')
plt.title("Average Layer Similarity Heatmap")
plt.savefig("imdb_similarity_heatmap.png", dpi=300, bbox_inches='tight')
plt.show()
plt.close()

thres = 0.9  # similarity threshold to prune redundant layers

ind_layer_to_del = set()

for i in range(len(avg_sim_matrix)):
    if i in ind_layer_to_del:
        continue
    for k in range(i + 1, len(avg_sim_matrix)):
        if k in ind_layer_to_del:
            continue
        if avg_sim_matrix[i][k] >= thres:
            ind_layer_to_del.add(k)
            print(f"Pruning layer {k} due to high similarity with layer {i} (sim={avg_sim_matrix[i][k]:.2f})")

print("Layers to delete:", sorted(ind_layer_to_del))

from transformers import BertForSequenceClassification
import torch.nn as nn

# Load your trained model again
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
model.load_state_dict(torch.load("best_model.pth"))
model.to(device)
model.eval()

# Convert encoder layers to a list
enc_list = list(model.bert.encoder.layer)

# Sort indices descending to delete without index shift issues
for idx in sorted(ind_layer_to_del, reverse=True):
    del enc_list[idx]

# Replace model's encoder layers with the pruned list
model.bert.encoder.layer = nn.ModuleList(enc_list)

# Adjust config for new number of layers
model.bert.config.num_hidden_layers = len(enc_list)

print(f"New number of encoder layers: {model.bert.config.num_hidden_layers}")

model.save_pretrained('./compressed_bert_imdb')
print("Compressed model saved to ./compressed_bert_imdb")

from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader
import torch

compressed_model = BertForSequenceClassification.from_pretrained('./compressed_bert_imdb')
compressed_model.to(device)
compressed_model.eval()

# 5. Evaluate the compressed model on validation and test sets
print("🔹 Evaluating Compressed Model:")

val_acc_compressed = evaluate(compressed_model, val_dl)
print(f"Validation Accuracy (Compressed Model): {val_acc_compressed:.4f}")

test_acc_compressed = evaluate(compressed_model, test_dl)
print(f"Test Accuracy (Compressed Model): {test_acc_compressed:.4f}")

"""# fine tuning using freeze and unfreeze //wrong method

"""

# prompt: fine tune baseline 1; bert base model

# ✅ Fine-tuning: Unfreeze the last 4 layers of BERT and add a lower learning rate
print("\n\n🔹 Fine-tuning: Unfreeze last 6 layers, lower LR")

# Re-initialize the model to ensure a clean state for fine-tuning
fine_tune_bert_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Freeze all layers first
for param in fine_tune_bert_model.bert.parameters():
    param.requires_grad = False

# Unfreeze the last 4 encoder layers
num_encoder_layers = fine_tune_bert_model.bert.config.num_hidden_layers
unfreeze_layers = 6 # Number of layers to unfreeze from the end
for i in range(num_encoder_layers - unfreeze_layers, num_encoder_layers):
    for param in fine_tune_bert_model.bert.encoder.layer[i].parameters():
        param.requires_grad = True

# Ensure the classifier layer is trainable
for param in fine_tune_bert_model.classifier.parameters():
     param.requires_grad = True

# Verify which layers are trainable
print("\nTrainable parameters after freezing/unfreezing:")
for name, param in fine_tune_bert_model.named_parameters():
    if param.requires_grad:
        print(f"  {name} (Trainable)")

# Fine-tuning training with a lower learning rate
print("\nStarting fine-tuning training...")
train(fine_tune_bert_model, train_dl, val_dl, test_dl, epochs=5)

print_layer_weights_biases(fine_tune_bert_model)

import argparse
import numpy as np
import torch
from sklearn.model_selection import train_test_split
import gc
import numpy as np
import torch.optim as optim
import torch.nn as nn
import string
from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig, BertTokenizer, BertModel, AutoTokenizer, AutoModelForTokenClassification
import pandas as pd
from torch.utils.data import TensorDataset, DataLoader, Dataset
import random
import os
import time
from torch import optim
from safetensors.torch import load_file,save_file
from torch.nn.functional import cosine_similarity
import pickle



"""# Distil Bert"""

print("\n🔹 Baseline 2: DistilBERT with Validation and Early Stopping")
distil_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
train_tok = tokenize_data(distil_tokenizer, train_data)
val_tok = tokenize_data(distil_tokenizer, val_data)
test_tok = tokenize_data(distil_tokenizer, test_data)

train_ds = IMDbDataset(train_tok)
val_ds = IMDbDataset(val_tok)
test_ds = IMDbDataset(test_tok)

train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=16)
test_dl = DataLoader(test_ds, batch_size=16)

distil_model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
train(distil_model, train_dl, val_dl, test_dl)

# prompt: for the above implemented distil bert model, check the number of layers and the weight and bias of each and every layer

# Check the number of layers in the DistilBERT model
print(f"\nNumber of DistilBERT layers: {distil_model.distilbert.config.n_layers}")

# Function to print weights and biases for each layer
def print_layer_weights_biases(model):
    print("\nChecking weights and biases for each layer:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Layer: {name}, Shape: {param.shape}")
            # Optionally print a small part of the data
            # print(f"  Weights/Biases sample: {param.data.cpu().numpy().flatten()[:5]}...")

print_layer_weights_biases(distil_model)

"""# Random Layer deletion"""

print("\n🔹 Baseline 3: BERT with Random Layer Deletion with Validation and Early Stopping")
compressed_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Remove random encoder layers
num_layers_to_remove = 6
all_layers = list(range(12))
layers_to_remove = sorted(random.sample(all_layers, num_layers_to_remove), reverse=True)
for i in layers_to_remove:
    del compressed_model.bert.encoder.layer[i]

train(compressed_model, train_dl, val_dl, test_dl)

print("✅ All baselines completed.")

# prompt: For the above random layer deletion bert model, check the number of layers and the weigths and biases of each layer

# Check the number of layers in the compressed BERT model
print(f"\nNumber of Compressed BERT layers: {len(compressed_model.bert.encoder.layer)}")

# Print weights and biases for the compressed BERT model
print_layer_weights_biases(compressed_model)